{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd\n",
    "import sklearn.metrics\n",
    "\n",
    "import datasets\n",
    "import units\n",
    "import models\n",
    "\n",
    "import config\n",
    "import logutil\n",
    "import utils\n",
    "from datasets.HICO import metadata\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_class_num = 117\n",
    "hoi_class_num = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluation(det_indices, pred_node_labels, node_labels, y_true, y_score, test=False):\n",
    "    np_pred_node_labels = pred_node_labels.data.cpu().numpy()\n",
    "    np_pred_node_labels_exp = np.exp(np_pred_node_labels)\n",
    "    np_pred_node_labels = np_pred_node_labels_exp/(np_pred_node_labels_exp+1)  # overflows when x approaches np.inf\n",
    "    np_node_labels = node_labels.data.cpu().numpy()\n",
    "\n",
    "    new_y_true = np.empty((2 * len(det_indices), action_class_num))\n",
    "    new_y_score = np.empty((2 * len(det_indices), action_class_num))\n",
    "    for y_i, (batch_i, i, j) in enumerate(det_indices):\n",
    "        new_y_true[2*y_i, :] = np_node_labels[batch_i, i, :]\n",
    "        new_y_true[2*y_i+1, :] = np_node_labels[batch_i, j, :]\n",
    "        new_y_score[2*y_i, :] = np_pred_node_labels[batch_i, i, :]\n",
    "        new_y_score[2*y_i+1, :] = np_pred_node_labels[batch_i, j, :]\n",
    "\n",
    "    y_true = np.vstack((y_true, new_y_true))\n",
    "    y_score = np.vstack((y_score, new_y_score))\n",
    "    return y_true, y_score\n",
    "\n",
    "# Weighted Loss\n",
    "def weighted_loss(output, target):\n",
    "    weight_mask = torch.autograd.Variable(torch.ones(target.size()))\n",
    "    if hasattr(args, 'cuda') and args.cuda:\n",
    "        weight_mask = weight_mask.cuda()\n",
    "    link_weight = args.link_weight if hasattr(args, 'link_weight') else 1.0\n",
    "    weight_mask += target * link_weight\n",
    "    return torch.nn.MultiLabelSoftMarginLoss(weight=weight_mask).cuda()(output, target)\n",
    "\n",
    "# Loss function\n",
    "def loss_fn(pred_adj_mat, adj_mat, pred_node_labels, node_labels, mse_loss, multi_label_loss, human_num=[], obj_num=[]):\n",
    "    np_pred_adj_mat = pred_adj_mat.data.cpu().numpy()\n",
    "    det_indices = list()\n",
    "    batch_size = pred_adj_mat.size()[0]\n",
    "    loss = 0\n",
    "    for batch_i in range(batch_size):\n",
    "        valid_node_num = human_num[batch_i] + obj_num[batch_i]\n",
    "        np_pred_adj_mat_batch = np_pred_adj_mat[batch_i, :, :]\n",
    "\n",
    "        if len(human_num) != 0:\n",
    "            human_interval = human_num[batch_i]\n",
    "            obj_interval = human_interval + obj_num[batch_i]\n",
    "        max_score = np.max([np.max(np_pred_adj_mat_batch), 0.01])\n",
    "        mean_score = np.mean(np_pred_adj_mat_batch)\n",
    "\n",
    "        batch_det_indices = np.where(np_pred_adj_mat_batch > 0.5)\n",
    "        for i, j in zip(batch_det_indices[0], batch_det_indices[1]):\n",
    "            # check validity for H-O interaction instead of O-O interaction\n",
    "            if len(human_num) != 0:\n",
    "                if i < human_interval and j < obj_interval:\n",
    "                    if j >= human_interval:\n",
    "                        det_indices.append((batch_i, i, j))\n",
    "\n",
    "        loss = loss + weighted_loss(pred_node_labels[batch_i, :valid_node_num].view(-1, action_class_num), node_labels[batch_i, :valid_node_num].view(-1, action_class_num))\n",
    "    return det_indices, loss\n",
    "\n",
    "# mean_avg_precision\n",
    "def compute_mean_avg_prec(y_true, y_score):\n",
    "    try:\n",
    "        avg_prec = sklearn.metrics.average_precision_score(y_true, y_score, average=None)\n",
    "        mean_avg_prec = np.nansum(avg_prec) / len(avg_prec)\n",
    "    except ValueError:\n",
    "        mean_avg_prec = 0\n",
    "\n",
    "    return mean_avg_prec\n",
    "\n",
    "# Train\n",
    "def train(train_loader, model, mse_loss, multi_label_loss, optimizer, epoch, logger):\n",
    "    batch_time = logutil.AverageMeter()\n",
    "    data_time = logutil.AverageMeter()\n",
    "    losses = logutil.AverageMeter()\n",
    "\n",
    "    y_true = np.empty((0, action_class_num))\n",
    "    y_score = np.empty((0, action_class_num))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    for i, (edge_features, node_features, adj_mat, node_labels, sequence_ids, det_classes, det_boxes, human_num, obj_num) in enumerate(train_loader):\n",
    "\n",
    "        data_time.update(time.time() - end_time)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        edge_features = utils.to_variable(edge_features, args.cuda)\n",
    "        node_features = utils.to_variable(node_features, args.cuda)\n",
    "        adj_mat = utils.to_variable(adj_mat, args.cuda)\n",
    "        node_labels = utils.to_variable(node_labels, args.cuda)\n",
    "\n",
    "        pred_adj_mat, pred_node_labels = model(edge_features, node_features, adj_mat, node_labels, human_num, obj_num, args)\n",
    "        det_indices, loss = loss_fn(pred_adj_mat, adj_mat, pred_node_labels, node_labels, mse_loss, multi_label_loss, human_num, obj_num)\n",
    "\n",
    "        # Log and back propagate\n",
    "        if len(det_indices) > 0:\n",
    "            y_true, y_score = evaluation(det_indices, pred_node_labels, node_labels, y_true, y_score)\n",
    "\n",
    "        losses.update(loss.data[0], edge_features.size()[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Measure elapsed time\n",
    "        batch_time.update(time.time() - end_time)\n",
    "        end_time = time.time()\n",
    "\n",
    "        if i % args.log_interval == 0:\n",
    "            mean_avg_prec = compute_mean_avg_prec(y_true, y_score)\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Mean Avg Precision {mean_avg_prec:.4f} ({mean_avg_prec:.4f})\\t'\n",
    "                  'Detected HOIs {y_shape}'\n",
    "                  .format(epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                          data_time=data_time, loss=losses, mean_avg_prec=mean_avg_prec, y_shape=y_true.shape))\n",
    "\n",
    "    mean_avg_prec = compute_mean_avg_prec(y_true, y_score)\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.log_value('train_epoch_loss', losses.avg)\n",
    "        logger.log_value('train_epoch_map', mean_avg_prec)\n",
    "\n",
    "    print('Epoch: [{0}] Avg Mean Precision {map:.4f}; Average Loss {loss.avg:.4f}; Avg Time x Batch {b_time.avg:.4f}'\n",
    "          .format(epoch, map=mean_avg_prec, loss=losses, b_time=batch_time))\n",
    "    \n",
    "# Validation\n",
    "def validate(val_loader, model, mse_loss, multi_label_loss, logger=None, test=False):\n",
    "    if args.visualize:\n",
    "        result_folder = os.path.join(args.tmp_root, 'results/HICO/detections/', 'top'+str(args.vis_top_k))\n",
    "        if not os.path.exists(result_folder):\n",
    "            os.makedirs(result_folder)\n",
    "\n",
    "    batch_time = logutil.AverageMeter()\n",
    "    losses = logutil.AverageMeter()\n",
    "\n",
    "    y_true = np.empty((0, action_class_num))\n",
    "    y_score = np.empty((0, action_class_num))\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (edge_features, node_features, adj_mat, node_labels, sequence_ids, det_classes, det_boxes, human_num, obj_num) in enumerate(val_loader):\n",
    "\n",
    "        edge_features = utils.to_variable(edge_features, args.cuda)\n",
    "        node_features = utils.to_variable(node_features, args.cuda)\n",
    "        adj_mat = utils.to_variable(adj_mat, args.cuda)\n",
    "        node_labels = utils.to_variable(node_labels, args.cuda)\n",
    "\n",
    "        pred_adj_mat, pred_node_labels = model(edge_features, node_features, adj_mat, node_labels, human_num, obj_num, args)\n",
    "        det_indices, loss = loss_fn(pred_adj_mat, adj_mat, pred_node_labels, node_labels, mse_loss, multi_label_loss, human_num, obj_num)\n",
    "\n",
    "        # Log\n",
    "        if len(det_indices) > 0:\n",
    "            losses.update(loss.data[0], len(det_indices))\n",
    "            y_true, y_score = evaluation(det_indices, pred_node_labels, node_labels, y_true, y_score, test=test)\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.log_interval == 0 and i > 0:\n",
    "            mean_avg_prec = compute_mean_avg_prec(y_true, y_score)\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Mean Avg Precision {mean_avg_prec:.4f} ({mean_avg_prec:.4f})\\t'\n",
    "                  'Detected HOIs {y_shape}'\n",
    "                  .format(i, len(val_loader), batch_time=batch_time,\n",
    "                          loss=losses, mean_avg_prec=mean_avg_prec, y_shape=y_true.shape))\n",
    "\n",
    "    mean_avg_prec = compute_mean_avg_prec(y_true, y_score)\n",
    "\n",
    "    print(' * Average Mean Precision {mean_avg_prec:.4f}; Average Loss {loss.avg:.4f}'\n",
    "          .format(mean_avg_prec=mean_avg_prec, loss=losses))\n",
    "\n",
    "    if logger is not None:\n",
    "        logger.log_value('test_epoch_loss', losses.avg)\n",
    "        logger.log_value('train_epoch_map', mean_avg_prec)\n",
    "\n",
    "    return 1.0 - mean_avg_prec\n",
    "\n",
    "# Get Indices\n",
    "def get_indices(pred_adj_mat, pred_node_labels, human_num, obj_num, det_class, det_box):\n",
    "\n",
    "    # Normalize adjacency matrix\n",
    "    pred_adj_mat_prob = torch.nn.Sigmoid()(pred_adj_mat)\n",
    "    np_pred_adj_mat = pred_adj_mat_prob.data.cpu().numpy()\n",
    "    # Normalize label outputs\n",
    "    pred_node_labels_prob = torch.nn.Sigmoid()(pred_node_labels)\n",
    "    np_pred_node_labels = pred_node_labels_prob.data.cpu().numpy()\n",
    "\n",
    "    hois = list()\n",
    "    threshold1 = 0\n",
    "    threshold2 = 0\n",
    "    threshold3 = 0\n",
    "    for h_idx in range(human_num):\n",
    "        bbox_h = det_box[h_idx]\n",
    "        h_class = det_class[h_idx]\n",
    "        for a_idx in range(len(metadata.action_classes)):\n",
    "            if np_pred_node_labels[h_idx, a_idx] < threshold1:\n",
    "                continue\n",
    "            max_score = -np.inf\n",
    "            selected = -1\n",
    "\n",
    "            for o_idx in range(human_num + obj_num):\n",
    "                obj_name = det_class[o_idx]\n",
    "                if a_idx not in metadata.obj_actions[obj_name]:\n",
    "                    continue\n",
    "                if np_pred_node_labels[h_idx, a_idx] < threshold2:\n",
    "                    continue\n",
    "                if np_pred_adj_mat[h_idx, o_idx] < threshold3:\n",
    "                    continue\n",
    "                score = np_pred_adj_mat[h_idx, o_idx] * np_pred_node_labels[h_idx, a_idx] * np_pred_node_labels[o_idx, a_idx]\n",
    "                bbox_o = det_box[o_idx]\n",
    "                rel_idx = metadata.action_to_obj_idx(obj_name, a_idx)\n",
    "                hois.append((h_class, obj_name, a_idx, (bbox_h, bbox_o, rel_idx, score), (np_pred_node_labels[h_idx, a_idx], np_pred_node_labels[o_idx, a_idx], np_pred_adj_mat[h_idx, o_idx])))\n",
    "\n",
    "    return hois\n",
    "\n",
    "# Generate Test result\n",
    "def gen_test_result(args, test_loader, model, mse_loss, multi_label_loss, img_index):\n",
    "    filtered_hoi = dict()\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    end = time.time()\n",
    "    total_idx = 0\n",
    "    obj_stats = dict()\n",
    "\n",
    "    filtered_hoi = dict()\n",
    "\n",
    "    for i, (edge_features, node_features, adj_mat, node_labels, sequence_ids, det_classes, det_boxes, human_nums, obj_nums) in enumerate(test_loader):\n",
    "        edge_features = utils.to_variable(edge_features, args.cuda)\n",
    "        node_features = utils.to_variable(node_features, args.cuda)\n",
    "        adj_mat = utils.to_variable(adj_mat, args.cuda)\n",
    "        node_labels = utils.to_variable(node_labels, args.cuda)\n",
    "        if sequence_ids[0] is 'HICO_test2015_00000396':\n",
    "            break\n",
    "\n",
    "        pred_adj_mat, pred_node_labels = model(edge_features, node_features, adj_mat, node_labels, human_nums, obj_nums, args)\n",
    "\n",
    "        for batch_i in range(pred_adj_mat.size()[0]):\n",
    "            sequence_id = sequence_ids[batch_i]\n",
    "            hois_test = get_indices(pred_adj_mat[batch_i], pred_node_labels[batch_i], human_nums[batch_i], obj_nums[batch_i],\n",
    "                        det_classes[batch_i], det_boxes[batch_i])\n",
    "            hois_gt = get_indices(adj_mat[batch_i], node_labels[batch_i], human_nums[batch_i],\n",
    "                                    obj_nums[batch_i],\n",
    "                                    det_classes[batch_i], det_boxes[batch_i])\n",
    "            for hoi in hois_test:\n",
    "                _, o_idx, a_idx, info, _ = hoi\n",
    "                if o_idx not in filtered_hoi.keys():\n",
    "                    filtered_hoi[o_idx] = dict()\n",
    "                if sequence_id not in filtered_hoi[o_idx].keys():\n",
    "                    filtered_hoi[o_idx][sequence_id] = list()\n",
    "                filtered_hoi[o_idx][sequence_id].append(info)\n",
    "\n",
    "        print(\"finished generating result from \" + sequence_ids[0] + \" to \" + sequence_ids[-1])\n",
    "\n",
    "    for obj_idx, save_info in filtered_hoi.items():\n",
    "        obj_start, obj_end = metadata.obj_hoi_index[obj_idx]\n",
    "        obj_arr = np.empty((obj_end - obj_start + 1, len(img_index)), dtype=np.object)\n",
    "        for row in range(obj_arr.shape[0]):\n",
    "            for col in range(obj_arr.shape[1]):\n",
    "                obj_arr[row][col] = []\n",
    "        for id, data_info in save_info.items():\n",
    "            col_idx = img_index.index(id)\n",
    "            for pair in data_info:\n",
    "                row_idx = pair[2]\n",
    "                bbox_concat = np.concatenate((pair[0], pair[1], [pair[3]]))\n",
    "                if len(obj_arr[row_idx][col_idx]) > 0:\n",
    "                    obj_arr[row_idx][col_idx] = np.vstack((obj_arr[row_idx][col_idx], bbox_concat))\n",
    "                else:\n",
    "                    obj_arr[row_idx][col_idx] = bbox_concat\n",
    "        sio.savemat(os.path.join(args.tmp_root, 'results', 'HICO', 'detections_' + str(obj_idx).zfill(2) + '.mat'), {'all_boxes': obj_arr})\n",
    "        print('finished saving for ' + str(obj_idx))\n",
    "\n",
    "    return\n",
    "\n",
    "# Add parse_arguments\n",
    "def parse_arguments():\n",
    "    # Parser check\n",
    "    def restricted_float(x, inter):\n",
    "        x = float(x)\n",
    "        if x < inter[0] or x > inter[1]:\n",
    "            raise argparse.ArgumentTypeError(\"%r not in range [1e-5, 1e-4]\"%(x,))\n",
    "        return x\n",
    "\n",
    "    paths = config.Paths()\n",
    "\n",
    "    # Path settings\n",
    "    parser = argparse.ArgumentParser(description='HICO dataset')\n",
    "    parser.add_argument('--project-root', default=paths.project_root, help='intermediate result path')\n",
    "    parser.add_argument('--tmp-root', default=paths.tmp_root, help='intermediate result path')\n",
    "    parser.add_argument('--data-root', default=paths.hico_data_root, help='data path')\n",
    "    parser.add_argument('--log-root', default=os.path.join(paths.log_root, 'hico/parsing'), help='log files path')\n",
    "    parser.add_argument('--resume', default=os.path.join(paths.tmp_root, 'checkpoints/hico/parsing'), help='path to latest checkpoint')\n",
    "    parser.add_argument('--visualize', action='store_true', default=False, help='Visualize final results')\n",
    "    parser.add_argument('--vis-top-k', type=int, default=1, metavar='N', help='Top k results to visualize')\n",
    "\n",
    "    # Optimization Options\n",
    "    parser.add_argument('--batch-size', type=int, default=1, metavar='N',\n",
    "                        help='Input batch size for training (default: 10)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='Enables CUDA training')\n",
    "    parser.add_argument('--epochs', type=int, default=0, metavar='N',\n",
    "                        help='Number of epochs to train (default: 10)')\n",
    "    parser.add_argument('--start-epoch', type=int, default=0, metavar='N',\n",
    "                        help='Index of epoch to start (default: 0)')\n",
    "    parser.add_argument('--link-weight', type=float, default=100, metavar='N',\n",
    "                        help='Loss weight of existing edges')\n",
    "    parser.add_argument('--lr', type=lambda x: restricted_float(x, [1e-5, 1e-2]), default=1e-5, metavar='LR',\n",
    "                        help='Initial learning rate [1e-5, 1e-2] (default: 1e-3)')\n",
    "    parser.add_argument('--lr-decay', type=lambda x: restricted_float(x, [.01, 1]), default=0.6, metavar='LR-DECAY',\n",
    "                        help='Learning rate decay factor [.01, 1] (default: 0.8)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                        help='SGD momentum (default: 0.9)')\n",
    "\n",
    "    # i/o\n",
    "    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
    "                        help='How many batches to wait before logging training status')\n",
    "    # Accelerating\n",
    "    parser.add_argument('--prefetch', type=int, default=0, help='Pre-fetching threads.')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "# Main function\n",
    "def main(args):\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    start_time = time.time()\n",
    "\n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    logger = logutil.Logger(os.path.join(args.log_root, timestamp))\n",
    "\n",
    "    # Load data\n",
    "    training_set, valid_set, testing_set, train_loader, valid_loader, test_loader, img_index = utils.get_hico_data(args)\n",
    "\n",
    "    # Get data size and define model\n",
    "    edge_features, node_features, adj_mat, node_labels, sequence_id, det_classes, det_boxes, human_num, obj_num = training_set[0]\n",
    "\n",
    "    edge_feature_size, node_feature_size = edge_features.shape[2], node_features.shape[1]\n",
    "    message_size = int(edge_feature_size/2)*2\n",
    "    model_args = {'model_path': args.resume, 'edge_feature_size': edge_feature_size, 'node_feature_size': node_feature_size, 'message_size': message_size, 'link_hidden_size': 512, 'link_hidden_layers': 2, 'link_relu': False, 'update_hidden_layers': 1, 'update_dropout': False, 'update_bias': True, 'propagate_layers': 3, 'hoi_classes': action_class_num, 'resize_feature_to_message_size': False}\n",
    "    model = models.GPNN_HICO(model_args)\n",
    "    del edge_features, node_features, adj_mat, node_labels\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    mse_loss = torch.nn.MSELoss(size_average=True)\n",
    "    multi_label_loss = torch.nn.MultiLabelSoftMarginLoss(size_average=True)\n",
    "    if args.cuda:\n",
    "        model = model.cuda()\n",
    "        mse_loss = mse_loss.cuda()\n",
    "        multi_label_loss = multi_label_loss.cuda()\n",
    "\n",
    "    loaded_checkpoint = datasets.utils.load_best_checkpoint(args, model, optimizer)\n",
    "    if loaded_checkpoint:\n",
    "        args, best_epoch_error, avg_epoch_error, model, optimizer = loaded_checkpoint\n",
    "\n",
    "    epoch_errors = list()\n",
    "    avg_epoch_error = np.inf\n",
    "    best_epoch_error = np.inf\n",
    "\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        logger.log_value('learning_rate', args.lr).step()\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, mse_loss, multi_label_loss, optimizer, epoch, logger)\n",
    "        # test on validation set\n",
    "        epoch_error = validate(valid_loader, model, mse_loss, multi_label_loss, logger)\n",
    "        epoch_errors.append(epoch_error)\n",
    "        if len(epoch_errors) == 2:\n",
    "            new_avg_epoch_error = np.mean(np.array(epoch_errors))\n",
    "            if avg_epoch_error - new_avg_epoch_error < 0.005:\n",
    "                print('Learning rate decrease')\n",
    "            avg_epoch_error = new_avg_epoch_error\n",
    "            epoch_errors = list()\n",
    "\n",
    "        if epoch % 5 == 0 and epoch > 0:\n",
    "            args.lr *= args.lr_decay\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = args.lr\n",
    "        is_best = True\n",
    "        best_epoch_error = min(epoch_error, best_epoch_error)\n",
    "        datasets.utils.save_checkpoint({'epoch': epoch + 1, 'state_dict': model.state_dict(),\n",
    "                                        'best_epoch_error': best_epoch_error, 'avg_epoch_error': avg_epoch_error,\n",
    "                                        'optimizer': optimizer.state_dict(), },\n",
    "                                       is_best=is_best, directory=args.resume)\n",
    "        print('best_epoch_error: {}, avg_epoch_error: {}'.format(best_epoch_error,  avg_epoch_error))\n",
    "\n",
    "    # For testing\n",
    "    loaded_checkpoint = datasets.utils.load_best_checkpoint(args, model, optimizer)\n",
    "    if loaded_checkpoint:\n",
    "        args, best_epoch_error, avg_epoch_error, model, optimizer = loaded_checkpoint\n",
    "\n",
    "    # validate(test_loader, model, mse_loss, multi_label_loss, test=True)\n",
    "    gen_test_result(args, test_loader, model, mse_loss, multi_label_loss, img_index)\n",
    "\n",
    "    print('Time elapsed: {:.2f}s'.format(time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
