{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetune.py: tune the resnet model\n",
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import torch.autograd\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda(non_blocking=True)\n",
    "        input_var = torch.autograd.Variable(input)\n",
    "        target_var = torch.autograd.Variable(target)\n",
    "\n",
    "        # compute output\n",
    "        feature, output = model(input_var)\n",
    "        loss = criterion(output, target_var.squeeze(1))\n",
    "\n",
    "        #################################################\n",
    "        ###################  feature  ###################\n",
    "        #################################################\n",
    "        # feature = feature.data.cpu().numpy()\n",
    "        # scipy.io.savemat(os.path.join(args.resume, '../features/train/{:05d}_{}.mat'.format(i, target.cpu().numpy()[0, 0])), {'feature': feature})\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        # prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        prec1 = accuracy(output, target)\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1, input.size(0))\n",
    "        # top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                data_time=data_time, loss=losses, top1=top1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate function\n",
    "def validate(val_loader, model, criterion, test=False):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.cuda(non_blocking=True)\n",
    "        input_var = torch.autograd.Variable(input, volatile=True)\n",
    "        target_var = torch.autograd.Variable(target, volatile=True)\n",
    "\n",
    "        # compute output\n",
    "        feature, output = model(input_var)\n",
    "        loss = criterion(output, target_var.squeeze(1))\n",
    "\n",
    "        #################################################\n",
    "        ###################  feature  ###################\n",
    "        #################################################\n",
    "        # feature = feature.data.cpu().numpy()\n",
    "        # if test:\n",
    "        #     scipy.io.savemat(os.path.join(args.resume, '../features/test/{:05d}_{}.mat'.format(i, target.cpu().numpy()[0, 0])), {'feature': feature})\n",
    "        # else:\n",
    "        #     scipy.io.savemat(os.path.join(args.resume, '../features/val/{:05d}_{}.mat'.format(i, target.cpu().numpy()[0, 0])), {'feature': feature})\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        # prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n",
    "        prec1 = accuracy(output, target)\n",
    "        losses.update(loss.data[0], input.size(0))\n",
    "        top1.update(prec1, input.size(0))\n",
    "        # top5.update(prec5[0], input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.4f} ({top1.avg:.4f})'.format(\n",
    "                i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.4f} Prec@5 {top5.avg:.4f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model parameters\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth'):\n",
    "    if not os.path.exists(args.resume):\n",
    "        os.makedirs(args.resume)\n",
    "    torch.save(state, os.path.join(args.resume, filename))\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(args.resume, filename), os.path.join(args.resume, 'model_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * (0.8 ** (epoch // 2))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target):\n",
    "    output = torch.nn.Softmax(dim=-1)(output)\n",
    "    correct_num = 0\n",
    "    output_np = output.data.cpu().numpy()\n",
    "    target_np = target.cpu().numpy()\n",
    "    for batch_i in range(target.size()[0]):\n",
    "        if np.argmax(output_np[batch_i, :]) == target_np[batch_i, 0]:\n",
    "            correct_num += 1\n",
    "    return float(correct_num) / target.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "def main(args):\n",
    "    best_prec1 = 0.0\n",
    "    args.distributed = args.world_size > 1\n",
    "    if args.distributed:\n",
    "        torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                             world_size=args.world_size)\n",
    "\n",
    "    # create model\n",
    "    if args.feature_type == 'vgg':\n",
    "        model = roi_feature_model.Vgg16(num_classes=len(metadata.action_classes))\n",
    "    elif args.feature_type == 'resnet':\n",
    "        model = roi_feature_model.Resnet152(num_classes=len(metadata.action_classes))\n",
    "    elif args.feature_type == 'densenet':\n",
    "        model = roi_feature_model.Densenet(num_classes=len(metadata.action_classes))\n",
    "    input_imsize = (224, 224)\n",
    "\n",
    "    if not args.distributed:\n",
    "        if args.feature_type.startswith('alexnet') or args.feature_type.startswith('vgg'):\n",
    "            model.features = torch.nn.DataParallel(model.features)\n",
    "            model.cuda()\n",
    "        else:\n",
    "            model = torch.nn.DataParallel(model).cuda()\n",
    "    else:\n",
    "        model.cuda()\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(os.path.join(args.resume, 'model_best.pth')):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(os.path.join(args.resume, 'model_best.pth'))\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(os.path.join(args.resume, 'model_best.pth')))\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # Data loading code\n",
    "    normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                 std=[0.229, 0.224, 0.225])\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    train_dataset = roi_feature_model.HICO(args.data, input_imsize, transform, 'train')\n",
    "    #val_dataset = roi_feature_model.HICO(args.data, input_imsize, transform, 'val')\n",
    "    test_dataset = roi_feature_model.HICO(args.data, input_imsize, transform, 'test')\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                               batch_size=args.batch_size, shuffle=True,\n",
    "                                               num_workers=args.workers, pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                              batch_size=args.batch_size, shuffle=True,\n",
    "                                              num_workers=args.workers, pin_memory=False)\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        if epoch == 0 or epoch >= 5:\n",
    "            # evaluate on validation set\n",
    "            prec1 = validate(test_loader, model, criterion)\n",
    "\n",
    "            # remember best prec@1 and save checkpoint\n",
    "            is_best = prec1 > best_prec1\n",
    "            best_prec1 = max(prec1, best_prec1)\n",
    "            print('Best precision: {:.03f}'.format(best_prec1))\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'arch': args.feature_type,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_prec1': best_prec1,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, is_best)\n",
    "\n",
    "    test_prec = validate(test_loader, model, criterion, test=True)\n",
    "    print('Testing precision: {:.04f}'.format(test_prec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    paths = \"home/yuxuan/gpnn/tmp/hico\"\n",
    "    feature_type = 'resnet'\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\n",
    "    parser.add_argument('--feature-type', default=feature_type, help='feature_type')\n",
    "    parser.add_argument('--data', metavar='DIR', default=paths, help='path to dataset')\n",
    "    parser.add_argument('-j', '--workers', default=1, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('--epochs', default=100, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                        help='manual epoch number (useful on restarts)')\n",
    "    parser.add_argument('-b', '--batch-size', default=32, type=int,\n",
    "                        metavar='N', help='mini-batch size (default: 256)')\n",
    "    parser.add_argument('--lr', '--learning-rate', default=1e-4, type=float,\n",
    "                        metavar='LR', help='initial learning rate')\n",
    "    parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                        help='momentum')\n",
    "    parser.add_argument('--weight-decay', '--wd', default=1e-3, type=float,\n",
    "                        metavar='W', help='weight decay (default: 1e-4)')\n",
    "    parser.add_argument('--print-freq', '-p', default=30, type=int,\n",
    "                        metavar='N', help='print frequency (default: 10)')\n",
    "    parser.add_argument('--resume', default=os.path.join(\"/home/yuxuan/gpnn/tmp\", 'checkpoints/hico/finetune_{}'.format(feature_type)), type=str, metavar='PATH',\n",
    "                        help='path to latest checkpoint (default: none)')\n",
    "    parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                        help='evaluate model on validation set')\n",
    "    parser.add_argument('--pretrained', dest='pretrained', default=True, action='store_true',\n",
    "                        help='use pre-trained model')\n",
    "    parser.add_argument('--world-size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist-url', default='tcp://224.66.41.62:23456', type=str,\n",
    "                        help='url used to set up distributed training')\n",
    "    parser.add_argument('--dist-backend', default='gloo', type=str,\n",
    "                        help='distributed backend')\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--feature-type FEATURE_TYPE] [--data DIR]\n",
      "                             [-j N] [--epochs N] [--start-epoch N] [-b N]\n",
      "                             [--lr LR] [--momentum M] [--weight-decay W]\n",
      "                             [--print-freq N] [--resume PATH] [-e]\n",
      "                             [--pretrained] [--world-size WORLD_SIZE]\n",
      "                             [--dist-url DIST_URL]\n",
      "                             [--dist-backend DIST_BACKEND]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/yuxuan/.local/share/jupyter/runtime/kernel-0452d29b-f680-40ac-a95e-5d5c57323f4e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxuan/anaconda3/envs/gpnn/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2886: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "args = parse_arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
