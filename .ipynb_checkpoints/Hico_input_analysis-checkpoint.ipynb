{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "#data = pickle.load(open( \"/home/yuxuan/gpnn/tmp/hico/processed/hico_data_background_49/HICO_train2015_00029234.p\", \"rb\" ))\n",
    "data = pickle.load(open( \"/home/yuxuan/gpnn/tmp/hico/processed/hico_data_background_49/HICO_test2015_00000001.p\", \"rb\" ))\n",
    "edge_features = np.load(\"/home/yuxuan/gpnn/tmp/hico/processed/hico_data_background_49/HICO_test2015_00000001_edge_features.npy\")\n",
    "node_features = np.load(\"/home/yuxuan/gpnn/tmp/hico/processed/hico_data_background_49/HICO_test2015_00000001_node_features.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_feature_size = edge_features.shape[2]\n",
    "node_feature_size = node_features.shape[1]\n",
    "print(edge_feature_size)\n",
    "print(node_feature_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('node features dimension', (3, 298))\n",
      "('edge features dimension:', (3, 3, 200))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'adj_mat': array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 1., 0.]]),\n",
       " 'boxes': array([[265.94937649, 302.89838073, 312.93957915, 347.02893622],\n",
       "        [312.8804666 , 303.3821591 , 354.10090454, 344.27008554],\n",
       "        [150.5163889 , 344.83840059, 374.46274961, 412.37458169],\n",
       "        [150.5163889 , 302.89838073, 374.46274961, 412.37458169],\n",
       "        [150.5163889 , 303.3821591 , 374.46274961, 412.37458169]]),\n",
       " 'classes': array([ 1,  1, 14,  0,  0]),\n",
       " 'human_num': 2,\n",
       " 'img_name': u'HICO_test2015_00000001',\n",
       " 'node_labels': array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0.]]),\n",
       " 'obj_num': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"node_labels\"].shape\n",
    "# 117 action classes\n",
    "\n",
    "print(\"node features dimension\", node_features.shape)\n",
    "print(\"edge features dimension:\", edge_features.shape)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_classes = ['adjust', 'assemble', 'block', 'blow', 'board', 'break', 'brush_with', 'buy', 'carry', 'catch',\n",
    "                   'chase', 'check', 'clean', 'control', 'cook', 'cut', 'cut_with', 'direct', 'drag', 'dribble',\n",
    "                   'drink_with', 'drive', 'dry', 'eat', 'eat_at', 'exit', 'feed', 'fill', 'flip', 'flush', 'fly',\n",
    "                   'greet', 'grind', 'groom', 'herd', 'hit', 'hold', 'hop_on', 'hose', 'hug', 'hunt', 'inspect',\n",
    "                   'install', 'jump', 'kick', 'kiss', 'lasso', 'launch', 'lick', 'lie_on', 'lift', 'light', 'load',\n",
    "                  'lose', 'make', 'milk', 'move', 'no_interaction', 'open', 'operate', 'pack', 'paint', 'park', 'pay',\n",
    "                  'peel', 'pet', 'pick', 'pick_up', 'point', 'pour', 'pull', 'push', 'race', 'read', 'release',\n",
    "                   'repair', 'ride', 'row', 'run', 'sail', 'scratch', 'serve', 'set', 'shear', 'sign', 'sip', 'sit_at',\n",
    "                   'sit_on', 'slide', 'smell', 'spin', 'squeeze', 'stab', 'stand_on', 'stand_under', 'stick', 'stir',\n",
    "                   'stop_at', 'straddle', 'swing', 'tag', 'talk_on', 'teach', 'text_on', 'throw', 'tie', 'toast',\n",
    "                   'train', 'turn', 'type_on', 'walk', 'wash', 'watch', 'wave', 'wear', 'wield', 'zip']\n",
    "len(action_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Information Flow of GPNN\n",
    "\n",
    "### Save the output of the result of GPNN: \n",
    "###### hico.gen_test_result(args, test_loader, model, mse_loss, multi_label_loss, img_index):\n",
    "\n",
    "use the GPNN to predict the hois of the trainset.   After that, use get_indices() in hico.py to calculate score of hoi.   An Array called filtered_hoi[o_idx][sequence_id].append(info), o_idx is equal to human number + object number, and sequence_is is image id. Infos like bounding box of human and object and score of hoi are appended in this array. o_idx is 80, it is hardcoded in the metadata.obj_hoi_index. There are totally 80 .mat file generated at the end.\n",
    "\n",
    "What is the content in each .mat file? obj_arr = np.empty((obj_end - obj_start + 1, len(img_index)), dtype=np.object). obj_end and obj_stat is the two element of obj_hoi classes, which i not fully understood what it means(may be it is filtering possible hoi). For each .mat file(more similarly for each set of action?), row number is the reduced obj_hoi number, col number is the predicted image number. There are following information stored in each cell: bounding box of human (4 cells), bounding box of object (4 cells), and hoi score (1 cell).\n",
    "\n",
    "My thought on how to read the result: there are 80 .mat file, which corresponds to a reduced obj_hoi_class region, we can find it by index. After that, the cells which contain information means that it has a meaningful rel_index, it is the index of true action in the reduced action lists (we reduce it using reduced obj_hoi list, this is maybe the reason why it is hard coded). Use this information, we can know the bounding box information and the predicted action label for each image.\n",
    "\n",
    "To generate the result, we need test_loader, model, and img_index as input. BTW image index is a list containing all image names which are predicted in this step.\n",
    "\n",
    "### Train the GNN:\n",
    "##### train(train_loader, model, mse_loss, multi_label_loss, optimizer, epoch, logger):\n",
    "##### models.GPNN_(HICO):\n",
    "\n",
    "train the GNN model, which is used to predict the node and edge of images.\n",
    "\n",
    "### Dataloader (train_loader, val_loader, test_loader)\n",
    "##### get_hico_data(args):\n",
    "\n",
    "Generally, we have 4 data loader. The file names of train set and validation set are from \"tmp/hico/trainval.txt\". The file names of test set are from \"tmp/hico/test.txt\". These 3 sets are used to train and test the GNN model. Besides, there are a set of images from \"tmp/hico/img_index.txt\". This set is used in the gen_rest_result function and produce result of hoi.\n",
    "\n",
    "The images are extracted and processed, apparently they are no more .jpg images and cannot be read by us. The root of features of images are \"/tmp/hico/processed/hico_data_background_49\". In that folder there are .p files for all images. To process the .jpg images into such .p features, the author uses a feature extraction pipeline.\n",
    "\n",
    "##### dataset.HICO(root, filenames)\n",
    "\n",
    "filenames are the image names in the data set. For each .p file, there are one dictionary which contains \"adj_mat\", \"boxes\", \"classes\", \"human_num\", \"img_name\", \"node_labels\" for each image. It returns such variables: edge_features, node_features, adj_mat, node_labels, sequence_id, det_classes, det_boxes, human_num, obj_num. edge_features and node_features are from the .npy file. sequence_id is basically the image index, det_boxes contain all bounding boxes, human_num and obj_num are number contained in the image.\n",
    "\n",
    "### Feature extraction\n",
    "#####  finetune.py\n",
    "\n",
    "Build resnet-152 model, train and test the model. Save checkpoint of the model if it acieves the best accuracy.\n",
    "\n",
    "##### extract_roi_features\n",
    "\n",
    "load the finetuned resnet-152 model from the last step. Read detection results under path \"/tmp/hico/hico_detect_trainvaltest_detections.pkl\"(I still not figured out how it is computed). This file basically contains information of all images.\n",
    "\n",
    "Using the information of  .pkl file we can save information about all detected classes, all detected boxes and all roi_features into image_name_features.npy files. roi_features are extrated from resized images using resnet.\n",
    "\n",
    "\n",
    "##### parse_features\n",
    "\n",
    "read the extracted roi_features and detected classes and boxes. Since for each image, we already know the number of objects and number of humans. There are some method to transfer the extracted features into nodes as well as edge features uasing roi size and numbers. Nodes use extracted features from single human or single object, edges use extracted features from condatenated bounding box features from human and object.\n",
    "\n",
    "After that, we compute the adjacency matrix and node labels from huaman number, object number, node number, and action_class_number for each image. We save those information in .p, edge- and node-features in .npy file seperately. Those file are stored under root \"/tmp/hico/processed/hico_data_background49\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
